{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from sklearn.manifold import TSNE\n",
    "from Bio.Cluster import kcluster\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input=\"TEVp-240412\"\n",
    "input_dataframe=pd.read_csv(f\"output/{input}/opt_binders/all.csv\")\n",
    "input_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# add scaffold name column\n",
    "def add_scaffold_name_column(filtered, prefix):\n",
    "    filtered[\"scaffold_name\"] = \"\"\n",
    "    \n",
    "    for index, row in filtered.iterrows():\n",
    "        path = row[\"model_path\"]\n",
    "        prefix = prefix\n",
    "        file_name = path.split(\"/\")[-1]\n",
    "        parts = file_name.split(prefix)[-1].split(\"_\")\n",
    "        \n",
    "        if len(parts) >= 5:\n",
    "            result = f\"{parts[0]}_{parts[1]}\"\n",
    "        else:\n",
    "            result = parts[0].split(\".\")[0]\n",
    "        \n",
    "        filtered.at[index, \"scaffold_name\"] = result\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "def repeat_rows_by_column_value(df, column_name, number):\n",
    "    unique_values = df[column_name].unique()\n",
    "    repeated_rows = []\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = df[df[column_name] == value]\n",
    "        num_repeats = min(number, subset.shape[0])\n",
    "        repeated_rows.extend([subset.iloc[i, :] for i in range(num_repeats)])\n",
    "\n",
    "    repeated_df = pd.DataFrame(repeated_rows)\n",
    "    return repeated_df\n",
    "\n",
    "#best_binders=add_scaffold_name_column(best_binders, input+\"_\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered = input_dataframe[(input_dataframe[\"rmsd\"]<1)&(input_dataframe[\"plddt\"]>0.85)&input_dataframe[\"i_pae\"]<6]\n",
    "filtered =input_dataframe[(input_dataframe[\"plddt\"]>0.90)&(input_dataframe[\"i_pae\"]<8)&(input_dataframe[\"rmsd\"]<1.5)]\n",
    "\n",
    "filtered = filtered.sort_values(by='plddt', ascending=False).drop_duplicates(\"model_path\").reset_index()#.head(9900)\n",
    "filtered=add_scaffold_name_column(filtered, input+\"_\")\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics on scaffolds\n",
    "scaffold_counts = filtered[\"scaffold_name\"].value_counts()\n",
    "total_unique_scaffolds = len(scaffold_counts)\n",
    "total_scaffold_instances = scaffold_counts.sum()\n",
    "\n",
    "print(\"Total unique scaffolds:\", total_unique_scaffolds)\n",
    "print(\"Total scaffold instances:\", total_scaffold_instances)\n",
    "print(\"\\nScaffold counts:\")\n",
    "print(scaffold_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "designs_per_scaffold = 200\n",
    "\n",
    "filtered=repeat_rows_by_column_value(filtered, \"scaffold_name\", designs_per_scaffold)\n",
    "#folder=f\"output/{input}/opt_binders/test\"\n",
    "#os.makedirs(folder, exist_ok=True)\n",
    "#for path in filtered[\"model_path\"]:\n",
    "#    !cp $path $folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good_scaffolds = [\"2_\",\"55_\",\"61_\",\"54_\",\"24_\",\"13_\",\"11_\"]\n",
    "\n",
    "# # Filter the DataFrame\n",
    "# #filtered = filtered[filtered['scaffold_name'].isin(good_scaffolds)]\n",
    "# filtered = filtered[~filtered['scaffold_name'].isin(good_scaffolds)]\n",
    "# filtered\n",
    "#for  scaff in good_scaffolds:\n",
    "#    filtered=filtered[scaff]\n",
    "\n",
    "# bad_scaffolds = [\"model_30\",\"model_42\", \"model_1-lcb3\"]\n",
    "\n",
    "# Filter the DataFrame\n",
    "# good = filtered[~filtered['scaffold_name'].isin(bad_scaffolds)]\n",
    "# good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[\"seq_split\"] = filtered[\"seq\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "#filtered[\"seq_split\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing cluster sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs=filtered[\"seq_split\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_statistics(input_list):\n",
    "    length_stats = {}\n",
    "    \n",
    "    for item in input_list:\n",
    "        item_length = len(item)\n",
    "        length_stats[item_length] = length_stats.get(item_length, 0) + 1\n",
    "    \n",
    "    return length_stats\n",
    "\n",
    "seqs_len=length_statistics(seqs)\n",
    "print(seqs_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters=25\n",
    "\n",
    "seqs=filtered[\"seq_split\"].to_list()\n",
    "#matrix = np.asarray([np.frombuffer(seq.encode(), dtype=np.uint8) for seq in seqs])\n",
    "max_length = max(len(seq) for seq in seqs)\n",
    "padded_seqs = [seq.ljust(max_length, 'N') for seq in seqs]\n",
    "matrix = np.asarray([np.frombuffer(seq.encode(), dtype=np.uint8) for seq in padded_seqs])\n",
    "clusterid, error, nfound = kcluster(matrix, nclusters=num_clusters)\n",
    "\n",
    "# Apply t-SNE to the matrix to reduce the dimensionality and visualize the sequences.\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embedded_matrix = tsne.fit_transform(matrix)\n",
    "\n",
    "# Create a scatter plot of the embedded points and label them with cluster IDs.\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_points = embedded_matrix[clusterid == cluster]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\")\n",
    "\n",
    "plt.title(f\"t-SNE Visualization of {input} best protein sequences\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "#plt.savefig(f\"output/{input}/filtered_sequences/tsne_binders.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the number of sequences in each cluster.\n",
    "cluster_counts = Counter(clusterid)\n",
    "sorted_cluster_counts = dict(sorted(cluster_counts.items()))\n",
    "for cluster, count in sorted_cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} sequences\")\n",
    "\n",
    "# Add cluster id to dataframe\n",
    "filtered[\"clusterid\"]=clusterid\n",
    "#filtered.to_csv(f\"output/{input}/filtered_sequences/2_filtered_binders_clus.csv\", index=False)\n",
    "\n",
    "# Calculate average cluster metrics\n",
    "average_metrics_by_cluster = filtered.groupby('clusterid').mean()\n",
    "#average_metrics_by_cluster.to_csv(f\"output/{input}/filtered_sequences/2_cluster_average.csv\", index=False)\n",
    "average_metrics_by_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metrics command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = f\"output/{input}/opt_binders/filtered.csv\" # Save filtered\n",
    "metric_path = f\"output/{input}/opt_binders/metrics.csv\" # Save filtered with metrics\n",
    "\n",
    "# Make filtered dataframe or append new sequences to the old one\n",
    "if os.path.exists(save_path):\n",
    "    print(\"reading existant dataframe...\")\n",
    "    existing_dataframe = pd.read_csv(save_path)\n",
    "    filtered_new = filtered[~filtered[\"model_path\"].isin(existing_dataframe[\"model_path\"])]\n",
    "    print(f\"existing dataframe of len: {len(existing_dataframe)}, new filtered: {len(filtered_new)}\")\n",
    "    existing_dataframe = pd.concat([existing_dataframe, filtered_new], ignore_index=True)\n",
    "    print(f\"final length: {len(existing_dataframe)}\")\n",
    "    existing_dataframe = existing_dataframe.sort_values(by='plddt', ascending=False)\n",
    "    #drop duplicates\n",
    "    existing_dataframe.to_csv(save_path, index=False)\n",
    "    existing_dataframe.to_csv(metric_path, index=False)\n",
    "\n",
    "else:\n",
    "    filtered.to_csv(save_path, index=False)\n",
    "    filtered.to_csv(metric_path, index=False)\n",
    "    existing_dataframe=filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_path=\"/home/tsatler/RFdif/ClusterProteinDesign/scripts/binder_design/output/{input}/opt_binders/binders/test\"\n",
    "#os.makedirs(test_path, exist_ok=True)\n",
    "\n",
    "#for pdb in filtered[\"model_path\"].head(10):\n",
    "#    !cp $pdb $test_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare input files for analysis script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_directory = f\"output/{input}/opt_binders/analysis_input\"\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "# Split the model_paths into batches\n",
    "model_paths = existing_dataframe[\"model_path\"]\n",
    "batches = [model_paths[i:i + batch_size] for i in range(0, len(model_paths), batch_size)]\n",
    "\n",
    "# Save each batch as a separate TXT file\n",
    "for i, batch in enumerate(batches):\n",
    "    save_path = os.path.join(save_directory, \"model_paths_\" + str(i) + \".txt\")\n",
    "    with open(save_path, \"w\") as file:\n",
    "        file.write(\"\\n\".join(batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analysis script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files=glob.glob(f\"{save_directory}/*txt\")\n",
    "array_limit=300//len(input_files)\n",
    "target_chain=\"A\"\n",
    "binder_chain=\"B\"\n",
    "xml_file=\"helper_scripts/metrics_calc.xml\"\n",
    "\n",
    "commands=[]\n",
    "\n",
    "for input_file in input_files:\n",
    "    with open(input_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    array_number = len(lines)-1\n",
    "\n",
    "    bash_arguments=f\"--output=/dev/null --array=0-{array_number}%{array_limit}\"\n",
    "    script_arguments=f\"{input_file} {target_chain} {binder_chain} {metric_path} {xml_file}\"\n",
    "\n",
    "    command = f\"sbatch {bash_arguments} helper_scripts/binder_analysis.sh {script_arguments}\"\n",
    "    print(command)\n",
    "    commands.append(command)\n",
    "\n",
    "print(f\"This will run {len(commands)} array scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the array bash script\n",
    "for command in commands:\n",
    "    subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "default"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
