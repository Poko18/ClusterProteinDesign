{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Cluster import kcluster\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import shutil\n",
    "import glob\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "### Analysis input\n",
    "input = \"TEVp-240412\"\n",
    "input_dataframe = pd.read_csv(f\"output/{input}/opt_binders/metrics.csv\")\n",
    "\n",
    "os.makedirs(f\"output/{input}/filtered_sequences/filtered_binders\", exist_ok=True)\n",
    "input_dataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "# add scaffold name column\n",
    "def add_scaffold_name_column(filtered, prefix):\n",
    "    filtered[\"scaffold_name\"] = \"\"\n",
    "\n",
    "    for index, row in filtered.iterrows():\n",
    "        path = row[\"model_path\"]\n",
    "        prefix = prefix\n",
    "        file_name = path.split(\"/\")[-1]\n",
    "        parts = file_name.split(prefix)[-1].split(\"_\")\n",
    "\n",
    "        if len(parts) >= 5:\n",
    "            result = f\"{parts[0]}_{parts[1]}\"\n",
    "        else:\n",
    "            result = parts[0].split(\".\")[0]\n",
    "\n",
    "        filtered.at[index, \"scaffold_name\"] = result\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def repeat_rows_by_column_value(df, column_name, number):\n",
    "    unique_values = df[column_name].unique()\n",
    "    repeated_rows = []\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = df[df[column_name] == value]\n",
    "        num_repeats = min(number, subset.shape[0])\n",
    "        repeated_rows.extend([subset.iloc[i, :] for i in range(num_repeats)])\n",
    "\n",
    "    repeated_df = pd.DataFrame(repeated_rows)\n",
    "    return repeated_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter dataframe\n",
    "tipical metrics:\n",
    "- plddt > 0.85-0.9\n",
    "- i_pae < 5\n",
    "\n",
    "additional metrics:\n",
    "- charge < -2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = input_dataframe[\n",
    "    (input_dataframe[\"plddt\"] > 0.90)\n",
    "    & (input_dataframe[\"i_pae\"] < 7)\n",
    "    & (input_dataframe[\"ddg\"] < -50)\n",
    "    & (input_dataframe[\"charge\"] < -2)\n",
    "    & (input_dataframe[\"ddg_dsasa_100\"] < -2.5)\n",
    "    &\n",
    "    # (input_dataframe[\"ddgscore_dsasa_100\"]<-2.5)&\n",
    "    (input_dataframe[\"shape_comp\"] > 0.5)\n",
    "    & (input_dataframe[\"vbuns_int\"] <= 1)\n",
    "    & (input_dataframe[\"cms\"] > 500)\n",
    "    & (input_dataframe[\"hyd_contacts\"] > 4)\n",
    "]\n",
    "# (input_dataframe[\"sap\"]<100)]#&\n",
    "# (input_dataframe[\"dG\"]<30)]\n",
    "\n",
    "filtered = filtered.sort_values(by=\"ddg_dsasa_100\", ascending=True).drop_duplicates(\n",
    "    \"seq\"\n",
    ")\n",
    "filtered.to_csv(\n",
    "    f\"output/{input}/filtered_sequences/0_filtered_binders.csv\", index=False\n",
    ")\n",
    "filtered.reset_index(inplace=True)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[\"rfd_id\"] = filtered[\"input_pdb\"].apply(\n",
    "    lambda x: x.split(input + \"_\")[-1].split(\".pdb\")[0]\n",
    ")\n",
    "filtered[\"rfd_id\"]\n",
    "\n",
    "\n",
    "rfd_id_counts = filtered[\"rfd_id\"].value_counts()\n",
    "\n",
    "# Get the number of unique 'rfd_id' values\n",
    "unique_rfd_ids_count = filtered[\"rfd_id\"].nunique()\n",
    "\n",
    "# Display the counts of each 'rfd_id' and the number of unique 'rfd_id' values\n",
    "print(\"Counts of each 'rfd_id':\")\n",
    "print(rfd_id_counts)\n",
    "print(\"\\nNumber of unique 'rfd_id' values:\", unique_rfd_ids_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = add_scaffold_name_column(filtered, input + \"_\")\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics on scaffolds\n",
    "scaffold_counts = filtered[\"scaffold_name\"].value_counts()\n",
    "total_unique_scaffolds = len(scaffold_counts)\n",
    "total_scaffold_instances = scaffold_counts.sum()\n",
    "\n",
    "print(\"Total unique scaffolds:\", total_unique_scaffolds)\n",
    "print(\"Total scaffold instances:\", total_scaffold_instances)\n",
    "print(\"\\nScaffold counts:\")\n",
    "print(scaffold_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_rows_by_column_value(df, column_name, number):\n",
    "    unique_values = df[column_name].unique()\n",
    "    repeated_rows = []\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = df[df[column_name] == value]\n",
    "        num_repeats = min(number, subset.shape[0])\n",
    "        repeated_rows.extend([subset.iloc[i, :] for i in range(num_repeats)])\n",
    "\n",
    "    repeated_df = pd.DataFrame(repeated_rows)\n",
    "    return repeated_df\n",
    "\n",
    "\n",
    "filtered = repeat_rows_by_column_value(filtered, \"rfd_id\", 200)\n",
    "filtered.to_csv(\n",
    "    f\"output/{input}/opt_binders/filtered_af2_rf2.csv\"\n",
    ")\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with ColabFold and Rosettafold2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fasta_sequence_from_pdb(pdb_file, output_folder=None):\n",
    "    ca_pattern = re.compile(\n",
    "        \"^ATOM\\s{2,6}\\d{1,5}\\s{2}CA\\s[\\sA]([A-Z]{3})\\s([\\s\\w])|^HETATM\\s{0,4}\\d{1,5}\\s{2}CA\\s[\\sA](MSE)\\s([\\s\\w])\"\n",
    "    )\n",
    "    aa3to1 = {\n",
    "        \"ALA\": \"A\",\n",
    "        \"VAL\": \"V\",\n",
    "        \"PHE\": \"F\",\n",
    "        \"PRO\": \"P\",\n",
    "        \"MET\": \"M\",\n",
    "        \"ILE\": \"I\",\n",
    "        \"LEU\": \"L\",\n",
    "        \"ASP\": \"D\",\n",
    "        \"GLU\": \"E\",\n",
    "        \"LYS\": \"K\",\n",
    "        \"ARG\": \"R\",\n",
    "        \"SER\": \"S\",\n",
    "        \"THR\": \"T\",\n",
    "        \"TYR\": \"Y\",\n",
    "        \"HIS\": \"H\",\n",
    "        \"CYS\": \"C\",\n",
    "        \"ASN\": \"N\",\n",
    "        \"GLN\": \"Q\",\n",
    "        \"TRP\": \"W\",\n",
    "        \"GLY\": \"G\",\n",
    "        \"MSE\": \"M\",\n",
    "    }\n",
    "    filename = os.path.basename(pdb_file).split(\".\")[0]\n",
    "    chain_dict = dict()\n",
    "    chain_list = []\n",
    "\n",
    "    with open(pdb_file, \"r\") as fp:\n",
    "        for line in fp:\n",
    "            if line.startswith(\"ENDMDL\"):\n",
    "                break\n",
    "            match_list = ca_pattern.findall(line)\n",
    "            if match_list:\n",
    "                resn = match_list[0][0] + match_list[0][2]\n",
    "                chain = match_list[0][1] + match_list[0][3]\n",
    "                if chain in chain_dict:\n",
    "                    chain_dict[chain] += aa3to1[resn]\n",
    "                else:\n",
    "                    chain_dict[chain] = aa3to1[resn]\n",
    "                    chain_list.append(chain)\n",
    "\n",
    "    fasta_sequence = f\">{filename}\\n\"\n",
    "    for i, chain in enumerate(chain_list):\n",
    "        fasta_sequence += chain_dict[chain]\n",
    "        if i < len(chain_list) - 1:\n",
    "            fasta_sequence += \":\"\n",
    "\n",
    "    if output_folder:\n",
    "        output_file = os.path.join(output_folder, f\"{filename}.fasta\")\n",
    "        with open(output_file, \"w\") as fp:\n",
    "            fp.write(fasta_sequence)\n",
    "\n",
    "    return chain_dict\n",
    "\n",
    "\n",
    "def process_a3m_file_rf2(\n",
    "    binder_sequence, sample_a3m_path, processed_file_name, verbose=False\n",
    "):\n",
    "    if os.path.isfile(sample_a3m_path):\n",
    "        with open(sample_a3m_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Replace old_binder_sequence with binder_sequence in the rest of the lines\n",
    "        if verbose:\n",
    "            print(lines[1])\n",
    "        old_binder_sequence = lines[1].strip().split(\"/\")[1]\n",
    "        if verbose:\n",
    "            print(old_binder_sequence)\n",
    "        for i in range(0, len(lines)):\n",
    "            lines[i] = lines[i].replace(old_binder_sequence, binder_sequence)\n",
    "\n",
    "            # Check if the line contains \"-\" and replace it accordingly\n",
    "            if not lines[i].startswith(\">\") and lines[i].split(\"/\")[1].startswith(\"-\"):\n",
    "                lines[i] = (\n",
    "                    lines[i].split(\"/\")[0] + \"/\" + \"-\" * len(binder_sequence) + \"\\n\"\n",
    "                )\n",
    "\n",
    "        # Save the processed lines to a new file\n",
    "        new_file_path = f\"{processed_file_name}\"\n",
    "        with open(new_file_path, \"w\") as new_file:\n",
    "            new_file.writelines(lines)\n",
    "\n",
    "\n",
    "def process_a3m_file_af2(binder_sequence, sample_a3m_path, processed_file_name):\n",
    "    if os.path.isfile(sample_a3m_path):\n",
    "        with open(sample_a3m_path, \"r\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Extract the target sequence from the first line\n",
    "        target_sequence_line = lines[0].strip()\n",
    "        if target_sequence_line.startswith(\"#\"):\n",
    "            target_length = int(target_sequence_line[1:].split(\",\")[0].strip())\n",
    "            # print(target_length)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The target sequence is not properly specified in the first line.\"\n",
    "            )\n",
    "\n",
    "        # Change line 0\n",
    "        lines[0] = f\"#{target_length},{len(binder_sequence)}\\t1,1\\n\"\n",
    "        # Change line 3\n",
    "        target_sequence = lines[2].strip()[:target_length]\n",
    "        old_binder_sequence = lines[2].strip()[target_length:]\n",
    "        lines[2] = target_sequence + binder_sequence + \"\\n\"\n",
    "\n",
    "        # Replace old_binder_sequence with binder_sequence in the rest of the lines\n",
    "        for i in range(3, len(lines)):\n",
    "            lines[i] = lines[i].replace(old_binder_sequence, binder_sequence)\n",
    "\n",
    "        # Write the processed lines to the output file\n",
    "        with open(processed_file_name, \"w\") as processed_file:\n",
    "            processed_file.writelines(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate MSA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for samples\n",
    "# make from whole sequence\n",
    "msa_folder_rf2 = f\"output/{input}/filtered_sequences/msa_inputs_rf2\"\n",
    "os.makedirs(msa_folder_rf2, exist_ok=True)\n",
    "\n",
    "sample_msa_folder_rf2 = f\"{msa_folder_rf2}/sample\"\n",
    "os.makedirs(sample_msa_folder_rf2, exist_ok=True)\n",
    "\n",
    "# AF2\n",
    "msa_folder_af2 = f\"output/{input}/filtered_sequences/msa_inputs_af2\"\n",
    "os.makedirs(msa_folder_rf2, exist_ok=True)\n",
    "\n",
    "sample_msa_folder_af2 = f\"{msa_folder_af2}/sample\"\n",
    "os.makedirs(sample_msa_folder_af2, exist_ok=True)\n",
    "\n",
    "# ?\n",
    "filtered_binders_path = f\"output/{input}/filtered_sequences/filtered_binders\"  # ??\n",
    "os.makedirs(filtered_binders_path, exist_ok=True)\n",
    "\n",
    "# copy pdbs to filtered binders\n",
    "for i, row in filtered.iterrows():\n",
    "    # Copy pdb in filtered folder\n",
    "    model_path = row[\"model_path\"]\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    new_model_path = f\"{filtered_binders_path}/{model_name}\"\n",
    "    shutil.copyfile(model_path, new_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[\"seq\"][0].split(\"/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If you performed RFdiffusion with partial protein sequence,\n",
    "here you can provide the whole sequence of the target protein.\n",
    "\"\"\"\n",
    "\n",
    "# same as the sequence for RFdiffusion\n",
    "target_whole_seq = filtered[\"seq\"][0].split(\"/\")[0]\n",
    "# or provide the whole sequence of the target protein\n",
    "# target_whole_seq=\"MNKSQLYPDSPLTDQDFNQLDQTVIEAARRQLVGRRFIELYGPLGRGMQSVFNDIFMESHEAKMDFQGSFDTEVESSRRVNYTIPMLYKDFVLYWRDLEQSKALDIPIDFSVAANAARDVAFLEDQMIFHGSKEFDIPGLMNVKGRLTHLIGNWYESGNAFQDIVEARNKLLEMNHNGPYALVLSPELYSLLHRVHKDTNVLEIEHVRELITAGVFQSPVLKGKSGVIVNTGRNNLDLAISEDFETAYLGEEGMNHPFRVYETVVLRIKRPAAICTLIDPEE\"\n",
    "\n",
    "\n",
    "sample_a3m_path_rf2 = f\"{msa_folder_rf2}/sample/sample.a3m\"\n",
    "sample_a3m_path_af2 = f\"{msa_folder_af2}/sample/sample.a3m\"\n",
    "\n",
    "\n",
    "random_binder_seq = filtered[\"seq\"][0].split(\"/\")[-1]\n",
    "print(f\"binder sequence: {random_binder_seq}\")\n",
    "print(f\"target sequence: {target_whole_seq}\")\n",
    "# Create fasta\n",
    "fasta_file_af2 = f\"{sample_msa_folder_af2}/fasta.fasta\"\n",
    "with open(f\"{fasta_file_af2}\", \"w\") as f:\n",
    "    # Write the sequence to the file in FASTA format\n",
    "    f.write(f\">sample\\n{target_whole_seq}:{random_binder_seq}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate MSA for AF2 and RF2\n",
    "\n",
    "\n",
    "### AF2\n",
    "input_pdb = filtered[\"model_path\"][0]\n",
    "slurm_args = \"\"  # \"--output=/dev/null\"\n",
    "cmd = f\"sbatch {slurm_args} helper_scripts/get_msa_colabfold.sh {input_pdb} {sample_msa_folder_af2}\"\n",
    "\n",
    "# Run the system command to execute the get_msa.py script\n",
    "# a3m_path_af2 = f\"{sample_msa_folder_af2}/{input_pdb.split('/')[-1].split('.')[0]}.a3m\"\n",
    "if not os.path.exists(sample_a3m_path_af2):\n",
    "    os.system(cmd)\n",
    "\n",
    "### RF2\n",
    "cmd_rf2 = f\"sbatch {slurm_args} helper_scripts/get_msa_rf.sh {fasta_file_af2} {sample_msa_folder_rf2}\"\n",
    "\n",
    "# Run the system command to execute the get_msa.py script\n",
    "# sample_a3m_path = f\"{sample_msa_folder_rf2}/sample.a3m\"\n",
    "if not os.path.exists(sample_a3m_path_rf2):\n",
    "    os.system(cmd_rf2)\n",
    "\n",
    "\n",
    "### WAITING\n",
    "# Wait until MSA is calculated...\n",
    "while not os.path.exists(sample_a3m_path_af2):\n",
    "    time.sleep(1)\n",
    "\n",
    "print(\"MSA calculation for AF2 is complete.\")\n",
    "\n",
    "while not os.path.exists(sample_a3m_path_rf2):\n",
    "    time.sleep(1)\n",
    "print(\"MSA calculation for RF2 is complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate other MSA files for RF2 and copy binders\n",
    "\n",
    "filtered[\"new_model_path\"] = None\n",
    "filtered[\"generated_a3m_rf2\"] = None\n",
    "\n",
    "filtered_binders_path = f\"output/{input}/filtered_sequences/filtered_binders\"\n",
    "os.makedirs(filtered_binders_path, exist_ok=True)\n",
    "\n",
    "binder_chain = \"B\"\n",
    "\n",
    "for i, row in filtered.iterrows():\n",
    "    # Copy pdb in filtered folder\n",
    "    model_path = row[\"model_path\"]\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    new_model_path = f\"{filtered_binders_path}/{model_name}\"\n",
    "    # shutil.copyfile(model_path, new_model_path)\n",
    "    binder_seq = row[\"seq\"].split(\"/\")[-1]\n",
    "\n",
    "    # binder_seq=generate_fasta_sequence_from_pdb(row[\"model_path\"])[\"B\"]\n",
    "    a3m_file_name = f\"{msa_folder_rf2}/{model_name.split('.')[0]}.a3m\"\n",
    "    sample_a3m_path = sample_a3m_path_rf2\n",
    "    process_a3m_file_rf2(binder_seq, sample_a3m_path, a3m_file_name)\n",
    "\n",
    "    # Update DataFrame with new values\n",
    "    filtered.loc[i, \"new_model_path\"] = new_model_path\n",
    "    filtered.loc[i, \"generated_a3m_rf2\"] = a3m_file_name\n",
    "\n",
    "# Check A3M files generated\n",
    "msa_a3m_inputs = glob.glob(f\"{msa_folder_rf2}/*a3m\")\n",
    "print(f\"There is {len(msa_a3m_inputs)} a3m files ready to be predicted with RF2!\")\n",
    "\n",
    "### Generate other MSA files for AF2 and copy binders\n",
    "\n",
    "filtered[\"new_model_path\"] = None\n",
    "filtered[\"generated_a3m_af2\"] = None\n",
    "\n",
    "\n",
    "binder_chain = \"B\"\n",
    "\n",
    "for i, row in filtered.iterrows():\n",
    "    # Copy pdb in filtered folder\n",
    "    model_path = row[\"model_path\"]\n",
    "    model_name = model_path.split(\"/\")[-1]\n",
    "    new_model_path = f\"{filtered_binders_path}/{model_name}\"\n",
    "    # shutil.copyfile(model_path, new_model_path)\n",
    "\n",
    "    # Generate msa\n",
    "    # binder_seq=generate_fasta_sequence_from_pdb(row[\"model_path\"])[\"B\"]\n",
    "    binder_seq = row[\"seq\"].split(\"/\")[-1]\n",
    "    a3m_file_name = f\"{msa_folder_af2}/{model_name.split('.')[0]}.a3m\"\n",
    "    # sample_a3m_path_af2=\"/home/tsatler/RFdif/ClusterProteinDesign/scripts/binder_design/output/5fmv_domain3-4/filtered_sequences/msa_inputs_af2/sample/sample.a3m\"\n",
    "    process_a3m_file_af2(binder_seq, sample_a3m_path_af2, a3m_file_name)\n",
    "\n",
    "    # Update DataFrame with new values\n",
    "    filtered.loc[i, \"new_model_path\"] = new_model_path\n",
    "    filtered.loc[i, \"generated_a3m_af2\"] = a3m_file_name\n",
    "\n",
    "# Check A3M files generated\n",
    "msa_a3m_inputs = glob.glob(f\"{msa_folder_af2}/*a3m\")\n",
    "print(f\"There is {len(msa_a3m_inputs)} a3m files ready to be predicted with AF2!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the prediction options as a dictionary with booleans\n",
    "prediction_options = {\n",
    "    \"colabfold\": True,  # Set to True if you want to run ColabFold\n",
    "    \"rosettafold2\": True,  # Set to True if you want to run RosettaFold2 (not currently used)\n",
    "}\n",
    "\n",
    "array_limit = 10\n",
    "binder_analysis = \"binder-second\"  # if you want to perform binder analysis\n",
    "msa_folder = f\"output/{input}/filtered_sequences\"\n",
    "\n",
    "array_number = len(msa_a3m_inputs)\n",
    "tools = \" \".join(\n",
    "    [tool for tool, should_run in prediction_options.items() if should_run]\n",
    ")\n",
    "print(tools)\n",
    "bash_arguments = f\"--output=/dev/null --array=0-{array_number}%{array_limit}\"\n",
    "script_arguments = f\"{msa_folder} {filtered_binders_path} {binder_analysis} {tools} \"\n",
    "\n",
    "command = (\n",
    "    f\"sbatch {bash_arguments} helper_scripts/predict_binders.sh {script_arguments}\"\n",
    ")\n",
    "print(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append and filter with scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# af2_dataframe_columns = ['plddt', 'pae', 'binder_plddt', 'target_plddt', 'pae_binder', 'pae_target', 'pae_int_tot', 'rmsd', 'name', 'model_id']\n",
    "# rf2_dataframe_columns = ['mean_plddt', 'pae_chain0_0', 'rmsd', 'name', 'model_id', 'plddt_target', 'plddt_binder', 'plddt', 'pae']\n",
    "\n",
    "# Open filtered dataframe\n",
    "filtered = pd.read_csv(f\"output/{input}/filtered_sequences/0_filtered_binders.csv\")\n",
    "filtered[\"model_id\"] = (\n",
    "    filtered[\"model_path\"].str.split(\"/\").str[-1].str.split(\".\").str[0]\n",
    ")\n",
    "\n",
    "# Open scores to append\n",
    "af2_scores = pd.read_csv(\n",
    "    f\"output/{input}/filtered_sequences/filtered_binders/scores/scores_af2.csv\"\n",
    ")\n",
    "rf2_scores = pd.read_csv(\n",
    "    f\"output/{input}/filtered_sequences/filtered_binders/scores/scores_rf2.csv\"\n",
    ")\n",
    "# Fix old model_ids\n",
    "af2_scores[\"model_id\"] = af2_scores[\"model_id\"].str.replace(r\"-AF2$\", \"\", regex=True)\n",
    "rf2_scores[\"model_id\"] = rf2_scores[\"model_id\"].str.replace(r\"-RF2$\", \"\", regex=True)\n",
    "\n",
    "# Columns to append\n",
    "af2_column_append = [\"binder_plddt\", \"pae_int_tot\", \"rmsd\"]\n",
    "rf2_column_append = [\"plddt_binder\", \"pae\", \"rmsd\"]\n",
    "\n",
    "# Prepare columns to add\n",
    "rf2_scores = rf2_scores[rf2_column_append + [\"model_id\"]].rename(\n",
    "    columns=lambda x: f\"rf_{x}\" if x != \"model_id\" else x\n",
    ")\n",
    "af2_scores = af2_scores[af2_column_append + [\"model_id\"]].rename(\n",
    "    columns=lambda x: f\"af_{x}\" if x != \"model_id\" else x\n",
    ")\n",
    "\n",
    "# Merge to filtered\n",
    "filtered = filtered.merge(af2_scores, on=\"model_id\", how=\"left\")\n",
    "filtered = filtered.merge(rf2_scores, on=\"model_id\", how=\"left\")\n",
    "filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = filtered[\n",
    "    (filtered[\"af_binder_plddt\"] > 80)\n",
    "    & (filtered[\"af_pae_int_tot\"] < 7)\n",
    "    & (filtered[\"af_rmsd\"] < 2)\n",
    "    &\n",
    "    # (filtered[\"rf_plddt_binder\"]>78)&\n",
    "    # (filtered[\"rf_pae\"]<15)&\n",
    "    (filtered[\"rf_rmsd\"] < 22)\n",
    "]\n",
    "\n",
    "filtered = filtered.sort_values(by=\"ddg_dsasa_100\", ascending=True).drop_duplicates(\n",
    "    \"seq\"\n",
    ")\n",
    "filtered.to_csv(\n",
    "    f\"output/{input}/filtered_sequences/1_filtered_binders.csv\", index=False\n",
    ")\n",
    "scaffold_counts = filtered[\"scaffold_name\"].value_counts()\n",
    "total_unique_scaffolds = len(scaffold_counts)\n",
    "total_scaffold_instances = scaffold_counts.sum()\n",
    "\n",
    "print(\"Total unique scaffolds:\", total_unique_scaffolds)\n",
    "print(\"Total scaffold instances:\", total_scaffold_instances)\n",
    "print(\"\\nScaffold counts:\")\n",
    "print(scaffold_counts)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[\"seq_split\"] = filtered[\"seq\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "\n",
    "num_clusters = 24\n",
    "\n",
    "seqs = filtered[\"seq_split\"].to_list()\n",
    "# matrix = np.asarray([np.frombuffer(seq.encode(), dtype=np.uint8) for seq in seqs])\n",
    "max_length = max(len(seq) for seq in seqs)\n",
    "padded_seqs = [seq.ljust(max_length, \"N\") for seq in seqs]\n",
    "matrix = np.asarray(\n",
    "    [np.frombuffer(seq.encode(), dtype=np.uint8) for seq in padded_seqs]\n",
    ")\n",
    "clusterid, error, nfound = kcluster(matrix, nclusters=num_clusters)\n",
    "\n",
    "# Apply t-SNE to the matrix to reduce the dimensionality and visualize the sequences.\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embedded_matrix = tsne.fit_transform(matrix)\n",
    "\n",
    "# Create a scatter plot of the embedded points and label them with cluster IDs.\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_points = embedded_matrix[clusterid == cluster]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\")\n",
    "\n",
    "plt.title(f\"t-SNE Visualization of {input} best protein sequences\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "# plt.savefig(f\"output/{input}/filtered_sequences/tsne_binders.png\")\n",
    "plt.show()\n",
    "\n",
    "# Add cluster id to dataframe\n",
    "filtered[\"clusterid\"] = clusterid\n",
    "# filtered.to_csv(f\"output/{input}/filtered_sequences/2_filtered_binders_clus.csv\", index=False)\n",
    "\n",
    "cluster_counts = Counter(filtered[\"clusterid\"])\n",
    "\n",
    "# Group the DataFrame by 'clusterid' and get unique scaffold names for each group\n",
    "unique_scaffold_names = filtered.groupby(\"clusterid\")[\"scaffold_name\"].unique()\n",
    "\n",
    "# Iterate over the groups and print both cluster counts and unique scaffold names\n",
    "for cluster_id, scaffold_names in unique_scaffold_names.items():\n",
    "    print(f\"Cluster {cluster_id}: {cluster_counts[cluster_id]} sequences\")\n",
    "    print(f'Scaffold Names: {\", \".join(scaffold_names)}')\n",
    "\n",
    "# Calculate average cluster metrics\n",
    "average_metrics_by_cluster = filtered.groupby(\"clusterid\").mean()\n",
    "# average_metrics_by_cluster.to_csv(f\"output/{input}/filtered_sequences/2_cluster_average.csv\", index=False)\n",
    "average_metrics_by_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output final dataframe and binders\n",
    "Code below will create folders with best [12, 24, 48 ..] binders and metrics.\n",
    "Make sure to change cluster ratios based on the results above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select number of final sequences and cluster ratios\n",
    "# total_numbers=[12, 24, 48]\n",
    "total_numbers = [24, 48, 96]\n",
    "\n",
    "cluster_ratios = [0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "cluster_ratios = np.full(num_clusters, 1 / num_clusters)\n",
    "\n",
    "# Function to calculate the number of sequences for each cluster based on the given ratios\n",
    "def calculate_sequences_ratios(total_sequences, cluster_ratios):\n",
    "    cluster_sequences = {}\n",
    "    total_ratio = sum(cluster_ratios)\n",
    "    remaining_sequences = total_sequences\n",
    "\n",
    "    for i, ratio in enumerate(cluster_ratios):\n",
    "        if i == len(cluster_ratios) - 1:\n",
    "            cluster_sequences[i] = remaining_sequences\n",
    "        else:\n",
    "            sequences_for_cluster = int(total_sequences * ratio / total_ratio)\n",
    "            cluster_sequences[i] = sequences_for_cluster\n",
    "            remaining_sequences -= sequences_for_cluster\n",
    "\n",
    "    return cluster_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for total_sequences in total_numbers:\n",
    "    # Calculate the number of sequences for each cluster\n",
    "    cluster_sequences = calculate_sequences_ratios(total_sequences, cluster_ratios)\n",
    "    print(f\"Total Sequences: {total_sequences}\")\n",
    "    print(cluster_sequences)\n",
    "\n",
    "    # Create a new dataframe with the final number of sequences for each cluster\n",
    "    final_sequences = []\n",
    "    for cluster_id, num_sequences in cluster_sequences.items():\n",
    "        cluster_data = filtered[filtered[\"clusterid\"] == cluster_id].iloc[\n",
    "            :num_sequences\n",
    "        ]\n",
    "        final_sequences.append(cluster_data)\n",
    "\n",
    "    final_dataframe = pd.concat(final_sequences)\n",
    "\n",
    "    # Save the dataframe to a CSV file\n",
    "    binder_pdbs_path = f\"output/{input}/filtered_sequences/final_results/binders_{total_sequences}/binder_{total_sequences}_pdbs\"\n",
    "    os.makedirs(binder_pdbs_path, exist_ok=True)\n",
    "    filename = f\"output/{input}/filtered_sequences/final_results/binders_{total_sequences}/final_binders_{total_sequences}.csv\"\n",
    "    final_dataframe.to_csv(filename, index=False)\n",
    "\n",
    "    for _, row in final_dataframe.iterrows():\n",
    "        pdb_path = row[\"model_path\"]\n",
    "\n",
    "        # filtered_binders_path=f\"output/{input}/filtered_sequences/colab_rf_eval_2domain_and_binder/filtered_binders\"s\n",
    "        # Copy pdb in filtered folder\n",
    "        # model_path=row[\"model_path\"]\n",
    "        # model_name=model_path.split(\"/\")[-1]\n",
    "        # pdb_path=f\"{filtered_binders_path}/{model_name}\"\n",
    "\n",
    "        if os.path.exists(pdb_path) and os.path.isfile(pdb_path):\n",
    "            pdb_filename = os.path.basename(pdb_path)\n",
    "            destination_path = os.path.join(binder_pdbs_path, pdb_filename)\n",
    "            shutil.copy(pdb_path, destination_path)\n",
    "            # print(f\"Copied {pdb_filename} to {destination_path}\")\n",
    "        else:\n",
    "            print(f\"File not found: {pdb_path}\")\n",
    "\n",
    "    print(f\"DataFrame for {total_sequences} sequences saved to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
