{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEW\n",
    "- skip threading and FastRelax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scaffolds/CD4-dom2-winners/51_3H', 'scaffolds/CD4-dom2-winners/lcb3']\n",
      "JOB: CD4-dom1-sample4\n",
      "TARGET: target_pdb/cd4-dom1-2-noHOH-1cdh.pdb\n",
      "HOTSPOTS: ppi.hotspot_res=[A34,A35,A43]\n",
      "Prefix does not exist yet\n",
      "\u001b[0mNumber of gpus already in use: 43\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "num_gpus=3 #How many gpus to use\n",
    "\n",
    "scaffold_dir=\"scaffolds/CD4-dom2-winners/\"\n",
    "separate_scaffold_folders=True\n",
    "\n",
    "prefix=\"CD4-dom1-sample4\"\n",
    "target_pdb=\"target_pdb/cd4-dom1-2-noHOH-1cdh.pdb\"\n",
    "hotspots='ppi.hotspot_res=[A34,A35,A43]'\n",
    "\n",
    "num_of_diffusions=10 # Number of RF diffusions to do per scaffold\n",
    "num_seqs=50 # How many MPNN sequences to generate per RF diffusion\n",
    "num_filtered_seqs=50 # How many of the MPNN generated sequences per RF diffusion to keep for AF2\n",
    "\n",
    "# Other parameters\n",
    "num_recycles=3 # AF2 recycles\n",
    "sampling_temp=0.0001 # ProteinMPNN sampling temperature\n",
    "\n",
    "def get_subdirectories(folder_path):\n",
    "    subdirectories = []\n",
    "    for entry in os.scandir(folder_path):\n",
    "        if entry.is_dir():\n",
    "            subdirectories.append(entry.path)\n",
    "    return subdirectories\n",
    "\n",
    "if separate_scaffold_folders:\n",
    "    scaffold_list=get_subdirectories(scaffold_dir)\n",
    "    scaffolds=len(scaffold_list)\n",
    "    print(scaffold_list)\n",
    "print(f\"JOB: {prefix}\")\n",
    "print(f\"TARGET: {target_pdb}\")\n",
    "print(f\"HOTSPOTS: {hotspots}\")\n",
    "\n",
    "def directory_exists(directory):\n",
    "    return os.path.isdir(directory)\n",
    "\n",
    "directory = f\"output/{prefix}\"\n",
    "if directory_exists(directory):\n",
    "    print(\"\"\"\\033[1;31;40m#####################\n",
    "PREFIX ALREADY EXISTS\n",
    "#####################\"\"\")\n",
    "else:\n",
    "    print(\"Prefix does not exist yet\")\n",
    "print(\"\\033[0mNumber of gpus already in use: \", end=\"\")\n",
    "!squeue | grep -c \"gpu\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs slurm array script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running sbatch --job-name=CD4-dom1-sample4_binder_docking --array=1-3 helper_scripts/binder_dock_nothread.sh 3 50 50 3 0.0001 scaffolds/CD4-dom2-winners/51_3H,scaffolds/CD4-dom2-winners/lcb3 CD4-dom1-sample4 target_pdb/cd4-dom1-2-noHOH-1cdh.pdb ppi.hotspot_res=[A34,A35,A43]\n",
      "Submitted batch job 389203\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "if separate_scaffold_folders:\n",
    "    if num_gpus%scaffolds==0:   #If number of gpus is divisible by the number of scaffolds, make it so that each scaffold has its own gpu(s)\n",
    "        gpus_per_scaffold = num_gpus//scaffolds\n",
    "        diffusions_per_gpu = num_of_diffusions//gpus_per_scaffold\n",
    "        for i in range(scaffolds):\n",
    "            # Setup arguments\n",
    "            scaffold_dir=scaffold_list[i]\n",
    "            script_arguments=f\"{diffusions_per_gpu} {num_seqs} {num_filtered_seqs} {num_recycles} {sampling_temp} {scaffold_dir} {prefix} {target_pdb} {hotspots}\"\n",
    "            bash_arguments=f\"--output={directory}/slurm-%A_%a.out --job-name={prefix}_binder_docking --array=1-{gpus_per_scaffold}\"\n",
    "            \n",
    "            command = f\"sbatch {bash_arguments} helper_scripts/binder_dock_nothread.sh {script_arguments}\"\n",
    "\n",
    "            # Run the array bash script\n",
    "            print(f\"Running {command}\")\n",
    "            subprocess.run(command, shell=True)\n",
    "    else:\n",
    "        num_per_gpu=num_of_diffusions//num_gpus\n",
    "        scaffold_string = ','.join(scaffold_list)   #Converts list of scaffold directories into a string for parsing in shell script\n",
    "        script_arguments=f\"{num_per_gpu} {num_seqs} {num_filtered_seqs} {num_recycles} {sampling_temp} {scaffold_string} {prefix} {target_pdb} {hotspots}\"\n",
    "        bash_arguments=f\"--output={directory}/slurm-%A_%a.out --job-name={prefix}_binder_docking --array=1-{num_gpus}\"\n",
    "            \n",
    "        command = f\"sbatch {bash_arguments} helper_scripts/binder_dock_nothread.sh {script_arguments}\"\n",
    "\n",
    "        # Run the array bash script\n",
    "        print(f\"Running {command}\")\n",
    "        subprocess.run(command, shell=True)\n",
    "\n",
    "else:\n",
    "    script_arguments=f\"{num_of_diffusions} {num_seqs} {num_filtered_seqs} {num_recycles} {sampling_temp} {scaffold_dir} {prefix} {target_pdb} {hotspots}\"\n",
    "    bash_arguments=f\"--output={directory}/slurm-%A_%a.out --job-name={prefix}_binder_docking --array=1-{num_gpus}\"\n",
    "    \n",
    "    command = f\"sbatch {bash_arguments} helper_scripts/binder_dock_nothread.sh {script_arguments}\"\n",
    "\n",
    "    # Run the array bash script\n",
    "    print(f\"Running {command}\")\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "#print(f\"command: {command}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          389186_1       gpu CD4-dom2  lhafner  R    1:29:49      1 compute-6-1\n",
      "          389187_1       gpu CD4-dom2  lhafner  R    1:29:49      1 compute-6-1\n",
      "          389203_1       gpu CD4-dom1  lhafner  R       9:29      1 compute-6-2\n",
      "          389203_2       gpu CD4-dom1  lhafner  R       9:29      1 compute-6-4\n",
      "          389203_3       gpu CD4-dom1  lhafner  R       9:29      1 compute-6-4\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "slurm-388518_2.out deleted.\n",
      "slurm-388516_2.out deleted.\n",
      "slurm-388710_1.out deleted.\n",
      "slurm-388779_2.out deleted.\n",
      "slurm-388707_1.out deleted.\n",
      "slurm-388514_1.out deleted.\n",
      "slurm-388671_2.out deleted.\n",
      "slurm-388709_1.out deleted.\n",
      "slurm-388681_2.out deleted.\n",
      "slurm-388516_1.out deleted.\n",
      "slurm-388667_2.out deleted.\n",
      "slurm-388778_2.out deleted.\n",
      "slurm-388778_1.out deleted.\n",
      "slurm-388690_1.out deleted.\n",
      "slurm-388780_1.out deleted.\n",
      "slurm-388545_2.out deleted.\n",
      "slurm-388686_2.out deleted.\n",
      "slurm-388708_1.out deleted.\n",
      "slurm-388779_1.out deleted.\n",
      "slurm-388781_2.out deleted.\n",
      "slurm-388517_1.out deleted.\n",
      "slurm-388781_1.out deleted.\n",
      "slurm-388780_2.out deleted.\n",
      "slurm-388681_1.out deleted.\n",
      "slurm-388686_1.out deleted.\n",
      "slurm-388517_2.out deleted.\n",
      "slurm-388782_1.out deleted.\n",
      "slurm-388669_2.out deleted.\n",
      "slurm-388706_1.out deleted.\n",
      "slurm-388669_1.out deleted.\n",
      "slurm-388514_2.out deleted.\n",
      "slurm-388518_1.out deleted.\n",
      "slurm-388688_2.out deleted.\n",
      "slurm-388667_1.out deleted.\n",
      "slurm-388515_1.out deleted.\n",
      "slurm-388515_2.out deleted.\n",
      "slurm-388690_2.out deleted.\n",
      "slurm-388688_1.out deleted.\n",
      "slurm-388545_1.out deleted.\n",
      "slurm-388782_2.out deleted.\n",
      "slurm-388671_1.out deleted.\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "### CAUTION ###\n",
    "###############\n",
    "##########################################\n",
    "### DO NOT RUN IF SLURM JOBS ARE RUNNING #\n",
    "##########################################\n",
    "# DELETES ALL slurm.out files. \n",
    "import os\n",
    "\n",
    "directory = os.getcwd()\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.startswith(\"slurm\") and filename.endswith(\".out\"):\n",
    "        try:\n",
    "            os.remove(os.path.join(directory, filename))\n",
    "            print(f\"{filename} deleted.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to delete {filename}. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
