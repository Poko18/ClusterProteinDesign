{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize\n",
    "imports & functions definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import subprocess\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Functions\n",
    "# add scaffold name column\n",
    "def add_scaffold_name_column(filtered, prefix):\n",
    "    filtered[\"scaffold_name\"] = \"\"\n",
    "\n",
    "    for index, row in filtered.iterrows():\n",
    "        path = row[\"model_path\"]\n",
    "        prefix = prefix\n",
    "        file_name = path.split(\"/\")[-1]\n",
    "        parts = file_name.split(prefix)[-1].split(\"_\")\n",
    "\n",
    "        if len(parts) >= 5:\n",
    "            result = f\"{parts[0]}_{parts[1]}_{parts[2]}\"\n",
    "        else:\n",
    "            result = parts[0].split(\".\")[0]\n",
    "\n",
    "        filtered.at[index, \"scaffold_name\"] = result\n",
    "\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def repeat_rows_by_column_value(df, column_name, number):\n",
    "    unique_values = df[column_name].unique()\n",
    "    repeated_rows = []\n",
    "\n",
    "    for value in unique_values:\n",
    "        subset = df[df[column_name] == value]\n",
    "        num_repeats = min(number, subset.shape[0])\n",
    "        repeated_rows.extend([subset.iloc[i, :] for i in range(num_repeats)])\n",
    "\n",
    "    repeated_df = pd.DataFrame(repeated_rows)\n",
    "    return repeated_df\n",
    "\n",
    "# best_binders=add_scaffold_name_column(best_binders, input+\"_\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"TEVp-240412\"\n",
    "input_dataframe = pd.read_csv(f\"output/{input}/opt_binders/all.csv\")\n",
    "input_dataframe\n",
    "\n",
    "filtered = input_dataframe[\n",
    "    (input_dataframe[\"rmsd\"] < 1) & (input_dataframe[\"plddt\"] > 0.9 )& (input_dataframe[\"i_pae\"] < 7) #Filters by specified metrics\n",
    "]\n",
    "# [(input_dataframe[\"plddt\"]>0.7)&(input_dataframe[\"i_pae\"]<8)&(input_dataframe[\"rmsd\"]<3)]\n",
    "\n",
    "filtered = filtered.sort_values(by=\"plddt\", ascending=False).drop_duplicates(\n",
    "    \"model_path\"    #Sorts by X metric\n",
    ")\n",
    "filtered = add_scaffold_name_column(filtered, input + \"_\") #Adds a column for scaffold name\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics on scaffolds\n",
    "scaffold_counts = filtered[\"scaffold_name\"].value_counts()\n",
    "total_unique_scaffolds = len(scaffold_counts)\n",
    "total_scaffold_instances = scaffold_counts.sum()\n",
    "\n",
    "print(\"Total unique scaffolds:\", total_unique_scaffolds)\n",
    "print(\"Total scaffold instances:\", total_scaffold_instances)\n",
    "print(\"\\nScaffold counts:\")\n",
    "print(scaffold_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check different scaffolds\n",
    "# filtered=repeat_rows_by_column_value(filtered, \"scaffold_name\", 1)\n",
    "# folder=f\"/home/tsatler/RFdif/ClusterProteinDesign/scripts/binder_design/output/{input}/opt_binders/test\"\n",
    "# os.makedirs(folder, exist_ok=True)\n",
    "# for path in filtered[\"model_path\"]:\n",
    "#     !cp $path $folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Filter by good scaffolds\n",
    "# good_scaffolds = [\"2_\",\"55_\",\"61_\",\"54_\",\"24_\",\"13_\",\"11_\"]\n",
    "# filtered = filtered[~filtered['scaffold_name'].isin(good_scaffolds)]\n",
    "# filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Redundant scaffolds\n",
    "designs_per_scaffold = 200\n",
    "\n",
    "filtered_per_scaffold = repeat_rows_by_column_value(filtered, \"scaffold_name\", designs_per_scaffold)\n",
    "filtered_per_scaffold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered[\"seq_split\"] = filtered[\"seq\"].apply(lambda x: x.split(\"/\")[-1])\n",
    "seqs = filtered[\"seq_split\"].to_list()\n",
    "\n",
    "num_clusters = 50\n",
    "\n",
    "seqs = filtered[\"seq_split\"].to_list()\n",
    "# matrix = np.asarray([np.frombuffer(seq.encode(), dtype=np.uint8) for seq in seqs])\n",
    "max_length = max(len(seq) for seq in seqs)\n",
    "padded_seqs = [seq.ljust(max_length, \"N\") for seq in seqs]\n",
    "matrix = np.asarray(\n",
    "    [np.frombuffer(seq.encode(), dtype=np.uint8) for seq in padded_seqs]\n",
    ")\n",
    "clusterid, error, nfound = kcluster(matrix, nclusters=num_clusters)\n",
    "\n",
    "# Apply t-SNE to the matrix to reduce the dimensionality and visualize the sequences.\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embedded_matrix = tsne.fit_transform(matrix)\n",
    "\n",
    "# Create a scatter plot of the embedded points and label them with cluster IDs.\n",
    "plt.figure(figsize=(10, 6))\n",
    "for cluster in range(num_clusters):\n",
    "    cluster_points = embedded_matrix[clusterid == cluster]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f\"Cluster {cluster}\")\n",
    "\n",
    "plt.title(f\"t-SNE Visualization of {input} best protein sequences\")\n",
    "plt.xlabel(\"t-SNE Dimension 1\")\n",
    "plt.ylabel(\"t-SNE Dimension 2\")\n",
    "plt.legend()\n",
    "# plt.savefig(f\"output/{input}/filtered_sequences/tsne_binders.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Print the number of sequences in each cluster.\n",
    "cluster_counts = Counter(clusterid)\n",
    "sorted_cluster_counts = dict(sorted(cluster_counts.items()))\n",
    "for cluster, count in sorted_cluster_counts.items():\n",
    "    print(f\"Cluster {cluster}: {count} sequences\")\n",
    "\n",
    "# Add cluster id to dataframe\n",
    "filtered[\"clusterid\"] = clusterid\n",
    "# filtered.to_csv(f\"output/{input}/filtered_sequences/2_filtered_binders_clus.csv\", index=False)\n",
    "\n",
    "# Calculate average cluster metrics\n",
    "average_metrics_by_cluster = filtered.groupby(\"clusterid\").mean()\n",
    "# average_metrics_by_cluster.to_csv(f\"output/{input}/filtered_sequences/2_cluster_average.csv\", index=False)\n",
    "average_metrics_by_cluster"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare metrics command and input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f\"output/{input}/opt_binders\", exist_ok=True)\n",
    "\n",
    "save_path = f\"output/{input}/opt_binders/filtered.csv\"  # Save filtered\n",
    "metric_path = f\"output/{input}/opt_binders/metrics.csv\"  # Save filtered with metrics\n",
    "\n",
    "# Make filtered dataframe or append new sequences to the old one\n",
    "if os.path.exists(save_path):\n",
    "    print(\"reading existant dataframe...\")\n",
    "    existing_dataframe = pd.read_csv(save_path)\n",
    "    filtered_new = filtered_per_scaffold[\n",
    "        ~filtered_per_scaffold[\"model_path\"].isin(existing_dataframe[\"model_path\"])\n",
    "    ]\n",
    "    print(\n",
    "        f\"existing dataframe of len: {len(existing_dataframe)}, new filtered: {len(filtered_new)}\"\n",
    "    )\n",
    "    existing_dataframe = pd.concat(\n",
    "        [existing_dataframe, filtered_new], ignore_index=True\n",
    "    )\n",
    "    print(f\"final length: {len(existing_dataframe)}\")\n",
    "    existing_dataframe = existing_dataframe.sort_values(by=\"plddt\", ascending=False)\n",
    "    # drop duplicates\n",
    "    existing_dataframe.to_csv(save_path, index=False)\n",
    "    existing_dataframe.to_csv(metric_path, index=False)\n",
    "\n",
    "else:\n",
    "    filtered_per_scaffold.to_csv(save_path, index=False)\n",
    "    filtered_per_scaffold.to_csv(metric_path, index=False)\n",
    "    existing_dataframe = filtered_per_scaffold\n",
    "\n",
    "save_directory = f\"output/{input}/opt_binders/analysis_input\"\n",
    "\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "# Split the model_paths into batches\n",
    "model_paths = existing_dataframe[\"model_path\"]\n",
    "batches = [\n",
    "    model_paths[i : i + batch_size] for i in range(0, len(model_paths), batch_size)\n",
    "]\n",
    "\n",
    "# Save each batch as a separate TXT file\n",
    "for i, batch in enumerate(batches):\n",
    "    save_path = os.path.join(save_directory, \"model_paths_\" + str(i) + \".txt\")\n",
    "    with open(save_path, \"w\") as file:\n",
    "        file.write(\"\\n\".join(batch))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run analysis script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = glob.glob(f\"{save_directory}/*txt\")\n",
    "array_limit = 300 // len(input_files)\n",
    "target_chain = \"A\"\n",
    "binder_chain = \"B\"\n",
    "xml_file = \"helper_scripts/metrics_calc.xml\"\n",
    "\n",
    "bash = f\"\"\"#!/bin/bash\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --exclude=compute-0-11\n",
    "\n",
    "# Load Anaconda environment\n",
    "source /home/tsatler/anaconda3/etc/profile.d/conda.sh\n",
    "\n",
    "input={input_file}\n",
    "input_pdbs=($(cat \"$input\"))\n",
    "pdb=${input_pdbs[$SLURM_ARRAY_TASK_ID]}\n",
    "\n",
    "target_chain=$2\n",
    "binder_chain=$3\n",
    "out_file=$4\n",
    "\n",
    "########################\n",
    "# Binder optimization\n",
    "########################\n",
    "echo \"Running binder analysis\"\n",
    "\n",
    "conda activate colabthread\n",
    "\n",
    "python helper_scripts/binder_analysis.py \\\n",
    "    $pdb \\\n",
    "    {target_chain} \\\n",
    "    {binder_chain} \\\n",
    "    $out_file \\\n",
    "    {xml_file}\n",
    "\"\"\"\n",
    "\n",
    "for input_file in input_files:\n",
    "    with open(input_file, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "    array_number = len(lines) - 1\n",
    "\n",
    "    bash_arguments = f\"--output=/dev/null --array=0-{array_number}%{array_limit}\"\n",
    "    script_arguments = (\n",
    "        f\"{input_file} {target_chain} {binder_chain} {metric_path} {xml_file}\"\n",
    "    )\n",
    "\n",
    "    command = (\n",
    "        f\"sbatch {bash_arguments} helper_scripts/binder_analysis.sh {script_arguments}\"\n",
    "    )\n",
    "    subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!squeue --me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
