{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run binder diffusion to hotspots!\n",
    "we used to thread diffused binders with their sequence and perform FastRelax before proteinMPNN, but we don't get more candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will diffuse 2 binders and design 20 sequences in total\n"
     ]
    }
   ],
   "source": [
    "prefix=\"TEVp-test\"\n",
    "scaffolds=\"/home/tsatler/RFdif/ClusterProteinDesign/scaffolds/rfdiff_scaffolds\"\n",
    "target=\"input/1lvb-clean.pdb\"\n",
    "hotspots='ppi.hotspot_res=[A172,A206,A210,A217]'\n",
    "\n",
    "num_of_diffusions=1 # Number of RF diffusions per job\n",
    "num_seqs=10 # How many MPNN sequences to generate per RF diffusion\n",
    "num_filtered_seqs=10 # How many of the MPNN generated sequences per RF diffusion to keep for AF2\n",
    "diff_steps = 50\n",
    "\n",
    "# Af2 Mpnn parameters\n",
    "num_recycles=3 # AF2 recycles\n",
    "sampling_temp=0.0001 # ProteinMPNN sampling temperature\n",
    "\n",
    "# Slurm parameters\n",
    "num_jobs=2 # Number of jobs to submit\n",
    "array_limit=3\n",
    "\n",
    "print(f\"This will diffuse {num_of_diffusions*num_jobs} binders and design {num_of_diffusions*num_jobs*num_seqs} sequences in total\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runs slurm array script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEVp-test\n"
     ]
    }
   ],
   "source": [
    "print(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_dir = os.path.dirname(target)\n",
    "target_base = os.path.basename(os.path.splitext(target)[0])\n",
    "ss = os.path.join(target_dir, f\"{target_base}_ss.pt\")\n",
    "adj = os.path.join(target_dir, f\"{target_base}_adj.pt\")\n",
    "\n",
    "output_folder = f\"output/{prefix}\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "rf_out_folder = f\"{output_folder}/rf_dock/{prefix}\"\n",
    "\n",
    "\n",
    "script = f\"\"\"#!/bin/bash\n",
    "#SBATCH --partition=gpu\n",
    "#SBATCH --gres=gpu:A40:1\n",
    "#SBATCH --ntasks=1\n",
    "#SBATCH --cpus-per-task=2\n",
    "#SBATCH --job-name={prefix}_binder_docking\n",
    "#SBATCH --array=0-{num_jobs-1}%{array_limit}\n",
    "\n",
    "set -e\n",
    "# Load Anaconda environment\n",
    "source /home/tsatler/anaconda3/etc/profile.d/conda.sh\n",
    "\n",
    "\n",
    "########################\n",
    "# RF diffusion\n",
    "########################\n",
    "\n",
    "# Generate ss, adj, and out_dir variables based on target_pdb\n",
    "# Check if ss and adj files exist\n",
    "if [ ! -f \"{ss}\" ] || [ ! -f \"{adj}\" ]; then\n",
    "  echo \"One or both of the SS and ADJ files do not exist. Running Python script to generate them...\"\n",
    "  \n",
    "  conda activate pyro\n",
    "  # Run Python script to generate ss and adj files\n",
    "  python helper_scripts/make_secstruc_adj.py --input_pdb \"{target}\" --out_dir \"{target_dir}\"\n",
    "else\n",
    "  echo \"SS and ADJ files already exist.\"\n",
    "fi\n",
    "\n",
    "\n",
    "echo Running RFdiffusion for docking scaffolds to the target...\n",
    "conda activate SE3nv\n",
    "\n",
    "rf_out={rf_out_folder}_$SLURM_ARRAY_TASK_ID/{prefix}_$SLURM_ARRAY_TASK_ID\n",
    "echo rf_out: $rf_out\n",
    "\n",
    "python /home/tsatler/RFdif/RFdiffusion/scripts/run_inference.py \\\n",
    "diffuser.T={diff_steps} \\\n",
    "scaffoldguided.target_path={target} \\\n",
    "inference.output_prefix=$rf_out \\\n",
    "scaffoldguided.scaffoldguided=True \\\n",
    "{hotspots} \\\n",
    "scaffoldguided.target_pdb=True \\\n",
    "scaffoldguided.target_ss={ss} \\\n",
    "scaffoldguided.target_adj={adj} \\\n",
    "scaffoldguided.scaffold_dir={scaffolds} \\\n",
    "'potentials.guiding_potentials=[\"type:binder_ROG,weight:3\",\"type:interface_ncontacts\",\"type:binder_distance_ReLU\"]' \\\n",
    "potentials.guide_scale=2 \\\n",
    "potentials.guide_decay=\"quadratic\" \\\n",
    "inference.num_designs={num_of_diffusions} \\\n",
    "denoiser.noise_scale_ca=0 \\\n",
    "denoiser.noise_scale_frame=0\n",
    "\n",
    "# Remove some files\n",
    "if [ -e \"$rf_out*.trb\" ]; then\n",
    "    rm \"$rf_out*.trb\" # delete trb files\n",
    "fi\n",
    "if [ -d \"{rf_out_folder}_$SLURM_ARRAY_TASK_ID/traj\" ]; then\n",
    "    rm -r \"{rf_out_folder}_$SLURM_ARRAY_TASK_ID/traj\" # delete trajectories\n",
    "fi\n",
    "\n",
    "\n",
    "########################\n",
    "# ProteinMPNN and AF2\n",
    "########################\n",
    "echo \"Running AFDesign with MPNN sampling\"\n",
    "\n",
    "conda activate colabthread\n",
    "\n",
    "input_files=($rf_out*.pdb)\n",
    "script=\"helper_scripts/colabinder.py\"\n",
    "\n",
    "for ((i=0; i<${{#input_files[@]}}; i++)); do\n",
    "  pdb_file=${{input_files[$i]}}\n",
    "  echo $pdb_file - ProteinMPNN and AF2\n",
    "  af_out={output_folder}/mpnn_af2/{prefix}_$SLURM_ARRAY_TASK_ID\n",
    "  python $script $pdb_file $af_out B A --sampling_temp {sampling_temp} \\\n",
    "  --num_recycles {num_recycles} --num_seqs {num_seqs} --num_filt_seq {num_filtered_seqs} \\\n",
    "  --results_dataframe {output_folder} --save_best_only\n",
    "done\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to a file\n",
    "script_file = f\"{output_folder}/run_docking.sh\"\n",
    "with open(script_file, \"w\") as f:\n",
    "    f.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 463345\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['sbatch', 'output/TEVp-test/run_docking.sh'], returncode=0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Submit the script\n",
    "subprocess.run([\"sbatch\", script_file])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "!squeue --me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
